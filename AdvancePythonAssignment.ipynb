{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HiramdeeP1603/advanced-python/blob/main/AdvancePythonAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part** **I - Process Automation**"
      ],
      "metadata": {
        "id": "ovb1ZTX74q8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) Create a file that contains 1000 lines of random strings"
      ],
      "metadata": {
        "id": "PmVHQdoN5rVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Sf-r9L434f1"
      },
      "outputs": [],
      "source": [
        "import random as r\n",
        "import string\n",
        "with open('result.txt','w') as file:\n",
        "  for i in range(1000):\n",
        "    letters=string.ascii_letters\n",
        "    rstring=''.join(r.choice(letters)for i in range(10))\n",
        "    file.write(rstring+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) Create a file that contains multiple lines of random strings with the file size must be of 5MB"
      ],
      "metadata": {
        "id": "Cs3y9Wt06D6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHEpsvxJ6tpe",
        "outputId": "3d9402b2-6c79-44a8-a2a9-04757992a7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 5242886 bytes (target was 5242880 bytes)\n"
          ]
        }
      ],
      "source": [
        "import random as r\n",
        "import string\n",
        "import os\n",
        "curr_size=0\n",
        "target_size=5*1024*1024\n",
        "with open('result2.txt','w') as file:\n",
        "  while curr_size < target_size:\n",
        "    letters=string.ascii_letters\n",
        "    rstring=''.join(r.choice(letters)for i in range(10))\n",
        "    file.write(rstring+'\\n')\n",
        "    curr_size += len(rstring) + 1\n",
        "actual_size = os.path.getsize('result2.txt')\n",
        "print(f\"File size: {actual_size} bytes (target was {target_size} bytes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(iii) Create 10 file that contains multiple lines of random strings with the file size of each must be of 5MB"
      ],
      "metadata": {
        "id": "-gAu7IQS7xcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "def generate_random_string(length=100):\n",
        "    letters = string.ascii_letters + string.digits\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "base_file_name = 'random_strings_file'\n",
        "target_size = 5 * 1024 * 1024  # 5MB in bytes\n",
        "\n",
        "for i in range(1, 11):\n",
        "    file_name = f'{base_file_name}_{i}.txt'\n",
        "    current_size = 0\n",
        "\n",
        "    with open(file_name, 'w') as file:\n",
        "        while current_size < target_size:\n",
        "            random_string = generate_random_string()\n",
        "            file.write(random_string + '\\n')\n",
        "            current_size += len(random_string) + 1\n",
        "\n",
        "    actual_size = os.path.getsize(file_name)\n",
        "    print(f\"Created {file_name} with size: {actual_size} bytes (target was {target_size} bytes)\")\n"
      ],
      "metadata": {
        "id": "8sDEJDjl76n7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764f28e0-06b9-447f-a9e3-c1f219f647e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created random_strings_file_1.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_2.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_3.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_4.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_5.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_6.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_7.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_8.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_9.txt with size: 5242910 bytes (target was 5242880 bytes)\n",
            "Created random_strings_file_10.txt with size: 5242910 bytes (target was 5242880 bytes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(iv) To create 5 files with size 1GB,2GB,3GM,4GB,5GB file contains multiple lines of random strings"
      ],
      "metadata": {
        "id": "gynI1K9x9KBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "def generate_random_string(length=100):\n",
        "    letters = string.ascii_letters + string.digits  # Include letters and digits\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "base_file_name = 'random_strings_file'\n",
        "target_sizes = [1 * 1024 * 1024 * 1024,  # 1GB\n",
        "                2 * 1024 * 1024 * 1024,  # 2GB\n",
        "                3 * 1024 * 1024 * 1024,  # 3GB\n",
        "                4 * 1024 * 1024 * 1024,  # 4GB\n",
        "                5 * 1024 * 1024 * 1024]  # 5GB\n",
        "\n",
        "for i, target_size in enumerate(target_sizes, start=1):\n",
        "    file_name = f'{base_file_name}_{i}GB.txt'\n",
        "    current_size = 0\n",
        "\n",
        "    with open(file_name, 'w') as file:\n",
        "        while current_size < target_size:\n",
        "            random_string = generate_random_string()\n",
        "            file.write(random_string + '\\n')\n",
        "            current_size += len(random_string) + 1\n",
        "\n",
        "    actual_size = os.path.getsize(file_name)\n",
        "    print(f\"Created {file_name} with size: {actual_size} bytes (target was {target_size} bytes)\")\n"
      ],
      "metadata": {
        "id": "N9XZyJYr9E2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(v) convert all the files the Q4 into uppercase"
      ],
      "metadata": {
        "id": "Wwi4YUL69C-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def convert_file_to_uppercase(input_file, output_file):\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            outfile.write(line.upper())\n",
        "\n",
        "file_sizes = [1, 2, 3, 4, 5]  # GB\n",
        "base_file_name = 'random_strings_file'\n",
        "\n",
        "for size in file_sizes:\n",
        "    input_file_name = f'{base_file_name}_{size}GB.txt'\n",
        "    output_file_name = f'{base_file_name}_{size}GB_uppercase.txt'\n",
        "\n",
        "    convert_file_to_uppercase(input_file_name, output_file_name)\n",
        "\n",
        "    actual_size = os.path.getsize(output_file_name)\n",
        "    print(f\"Created {output_file_name} with size: {actual_size} bytes (converted to uppercase)\")\n"
      ],
      "metadata": {
        "id": "TFgiMtRo9r_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(vii) WAP to automatically download 10 pics of cat from google"
      ],
      "metadata": {
        "id": "Wv9cbhN1-Pd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google_images_download\n",
        "from google_images_download import google_images_download\n",
        "\n",
        "response = google_images_download.googleimagesdownload()\n",
        "search_queries = [\"cat\"]\n",
        "\n",
        "# Configure download parameters\n",
        "download_args = {\n",
        "    \"keywords\": \"cat\",\n",
        "    \"limit\": 10,  # Number of images to download\n",
        "    \"format\": \"jpg\",  # Image format\n",
        "    \"output_directory\": \"downloaded_images\",  # Directory to save the images\n",
        "    \"no_directory\": True,  # Save images directly in the output directory\n",
        "}\n",
        "\n",
        "# Download images\n",
        "paths = response.download(download_args)\n",
        "print(paths)"
      ],
      "metadata": {
        "id": "vG_74R0N-VUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(viii) WAP to download 10 videos of machine learning from youtube"
      ],
      "metadata": {
        "id": "8-Mz7cl3_GBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube\n",
        "from pytube import YouTube\n",
        "\n",
        "def download_youtube_video(url, output_path):\n",
        "    try:\n",
        "        yt = YouTube(url)\n",
        "        video = yt.streams.filter(file_extension='mp4', progressive=True).first()\n",
        "        video.download(output_path)\n",
        "        print(f\"Downloaded: {yt.title}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {str(e)}\")\n",
        "\n",
        "video_urls = [\n",
        "    \"https://www.youtube.com/watch?v=qMFKsMeE6fM\",\n",
        "    \"https://www.youtube.com/watch?v=_Z9TRANg4c0\",\n",
        "    \"https://www.youtube.com/watch?v=ZV4ZKvcHh1k\",\n",
        "    \"https://www.youtube.com/watch?v=-58kO_zYUGE\",\n",
        "    \"https://www.youtube.com/watch?v=Wo5dMEP_BbI\",\n",
        "    \"https://www.youtube.com/watch?v=pDa6Tm2lDNc\",\n",
        "    \"https://www.youtube.com/watch?v=0auXlxaOun0\",\n",
        "    \"https://www.youtube.com/watch?v=8z03GKA9g4o\",\n",
        "    \"https://www.youtube.com/watch?v=KTNqXwkLuVM\",\n",
        "    \"https://www.youtube.com/watch?v=ukzFI9rgwfU\"\n",
        "]\n",
        "\n",
        "output_directory = \"downloaded_videos\"\n",
        "\n",
        "for url in video_urls:\n",
        "    download_youtube_video(url, output_directory)\n"
      ],
      "metadata": {
        "id": "vDInUD2m_WLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ix) Convert the Q8 videos into audio files"
      ],
      "metadata": {
        "id": "NCbwWZzw_7-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "\n",
        "def download_and_convert_video(url, output_directory):\n",
        "    try:\n",
        "        yt = YouTube(url)\n",
        "        video_stream = yt.streams.filter(file_extension='mp4', progressive=True).first()\n",
        "        video_filename = video_stream.download(output_directory)\n",
        "\n",
        "        video_clip = VideoFileClip(video_filename)\n",
        "        audio_filename = os.path.splitext(os.path.basename(video_filename))[0] + \".mp3\"\n",
        "        audio_path = os.path.join(output_directory, audio_filename)\n",
        "        video_clip.audio.write_audiofile(audio_path)\n",
        "\n",
        "        print(f\"Downloaded and converted: {yt.title} -> {audio_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "\n",
        "video_urls = [\n",
        "    \"https://www.youtube.com/watch?v=qMFKsMeE6fM\",\n",
        "    \"https://www.youtube.com/watch?v=_Z9TRANg4c0\",\n",
        "    \"https://www.youtube.com/watch?v=ZV4ZKvcHh1k\",\n",
        "    \"https://www.youtube.com/watch?v=-58kO_zYUGE\",\n",
        "    \"https://www.youtube.com/watch?v=Wo5dMEP_BbI\",\n",
        "    \"https://www.youtube.com/watch?v=pDa6Tm2lDNc\",\n",
        "    \"https://www.youtube.com/watch?v=0auXlxaOun0\",\n",
        "    \"https://www.youtube.com/watch?v=8z03GKA9g4o\",\n",
        "    \"https://www.youtube.com/watch?v=KTNqXwkLuVM\",\n",
        "    \"https://www.youtube.com/watch?v=ukzFI9rgwfU\"\n",
        "]\n",
        "\n",
        "output_directory = \"converted_audio\"\n",
        "\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for url in video_urls:\n",
        "    download_and_convert_video(url, output_directory)\n"
      ],
      "metadata": {
        "id": "m8NZez4RAR2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(X). Create an automated pipeline using multi-threading for:\n",
        "“Automatic Download of 100 Videos from YouTube” → “Convert it to Audio”"
      ],
      "metadata": {
        "id": "8TMfzdHpucj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pytube import YouTube\n",
        "from moviepy.editor import VideoFileClip\n",
        "import threading\n",
        "\n",
        "# Directory to save downloaded videos and extracted audio\n",
        "video_dir = \"videos\"\n",
        "audio_dir = \"audio\"\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "os.makedirs(audio_dir, exist_ok=True)\n",
        "\n",
        "# List of YouTube video URLs (for example purposes, replace with actual URLs)\n",
        "video_urls = [\n",
        "    \"https://www.youtube.com/watch?v=your_video_id1\",\n",
        "    \"https://www.youtube.com/watch?v=your_video_id2\",\n",
        "    # Add more URLs here\n",
        "]\n",
        "\n",
        "# Function to download a single video\n",
        "def download_video(url):\n",
        "    try:\n",
        "        yt = YouTube(url)\n",
        "        stream = yt.streams.filter(progressive=True, file_extension='mp4').first()\n",
        "        video_path = stream.download(output_path=video_dir)\n",
        "        return video_path\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to convert a single video to audio\n",
        "def convert_to_audio(video_path):\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        audio_path = os.path.join(audio_dir, os.path.splitext(os.path.basename(video_path))[0] + '.mp3')\n",
        "        clip.audio.write_audiofile(audio_path)\n",
        "        clip.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to convert {video_path} to audio: {e}\")\n",
        "\n",
        "# Function to process a single URL\n",
        "def process_url(url):\n",
        "    video_path = download_video(url)\n",
        "    if video_path:\n",
        "        convert_to_audio(video_path)\n",
        "\n",
        "# Create and start threads\n",
        "threads = []\n",
        "for url in video_urls:\n",
        "    thread = threading.Thread(target=process_url, args=(url,))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(\"Download and conversion completed.\")\n"
      ],
      "metadata": {
        "id": "fZpffLkIuwjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(XII). Create an automated pipeline using multi-threading for: “Automatic Download of 500 images of Dog from\n",
        "GoogleImages” → “Rescale it to 50%”.\n"
      ],
      "metadata": {
        "id": "rJ9Q-qN3v-eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google_images_search\n",
        "\n",
        "import os\n",
        "from google_images_search import GoogleImagesSearch\n",
        "from PIL import Image\n",
        "import threading\n",
        "\n",
        "# Ensure you have your API key and CX set up\n",
        "api_key = 'your_api_key'\n",
        "cx = 'your_cx'\n",
        "\n",
        "gis = GoogleImagesSearch(api_key, cx)\n",
        "\n",
        "# Directory to save downloaded images\n",
        "image_dir = \"images\"\n",
        "rescaled_dir = \"rescaled_images\"\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "os.makedirs(rescaled_dir, exist_ok=True)\n",
        "\n",
        "# Search parameters\n",
        "search_params = {\n",
        "    'q': 'dog',\n",
        "    'num': 100,  # Adjust if needed; we'll run this multiple times\n",
        "    'fileType': 'jpg',\n",
        "    'imgType': 'photo',\n",
        "    'imgSize': 'medium',\n",
        "    'safe': 'off'\n",
        "}\n",
        "\n",
        "# Function to download images\n",
        "def download_images(search_params, start_index):\n",
        "    search_params['start'] = start_index\n",
        "    gis.search(search_params=search_params, path_to_dir=image_dir)\n",
        "\n",
        "# Function to rescale images\n",
        "def rescale_image(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        img = img.resize((img.width // 2, img.height // 2))\n",
        "        rescaled_path = os.path.join(rescaled_dir, os.path.basename(image_path))\n",
        "        img.save(rescaled_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to rescale {image_path}: {e}\")\n",
        "\n",
        "# Function to process downloading and rescaling\n",
        "def process_images(start_index):\n",
        "    download_images(search_params, start_index)\n",
        "    for img_file in os.listdir(image_dir):\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        rescale_image(img_path)\n",
        "\n",
        "# Create and start threads for downloading and rescaling\n",
        "threads = []\n",
        "# Google Images API restricts results to 100 images per query. So, we need multiple queries.\n",
        "for i in range(0, 500, 100):\n",
        "    thread = threading.Thread(target=process_images, args=(i,))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(\"Image downloading and rescaling completed.\")\n"
      ],
      "metadata": {
        "id": "aM4N-ZezwFcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part II - Data Analytics**"
      ],
      "metadata": {
        "id": "VRlNuru-AVOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q12 Create a random dataset of 100 rows and 30 columns. All the values are defined between [1,200].  Perform the following operations:\n",
        "\n",
        "(i) Replace all the values with NA in the dataset defined between [10, 60]. Print the count of number\n",
        "rows having missing values.\n",
        "\n",
        "(ii) Replace all the NA values with the average of the column value.  \n",
        "\n",
        "(iii) Find the Pearson correlation among all the columns and plot heat map. Also select those columns\n",
        "having correlation <=0.7.\n",
        "\n",
        "(iv) Normalize all the values in the dataset between 0 and 10.\n",
        "\n",
        "(v) Replace all the values in the dataset with 1 if value <=0.5 else with 0."
      ],
      "metadata": {
        "id": "89GtWZWIAgMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a random dataset of 100 rows and 30 columns with values between 1 and 200\n",
        "#np.random.seed(42)\n",
        "data = np.random.randint(1, 201, size=(100, 30))\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Replace all values with NA in the dataset defined between [10, 60]\n",
        "df.replace(to_replace=range(10, 61), value=np.nan, inplace=True)\n",
        "\n",
        "# Step 3: Print the count of number of rows having missing values\n",
        "rows_with_na = df.isna().any(axis=1).sum()\n",
        "print(f\"Number of rows with missing values: {rows_with_na}\")\n",
        "\n",
        "# Step 4: Replace all the NA values with the average of the column value\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Step 5: Find the Pearson correlation among all the columns\n",
        "correlation_matrix = df.corr(method='pearson')\n",
        "\n",
        "# Step 6: Plot heat map of the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={'size': 4})\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Select those columns having correlation <= 0.7\n",
        "selected_columns = correlation_matrix.columns[(correlation_matrix <= 0.7).all()]\n",
        "print(f\"Columns with correlation <= 0.7: {selected_columns}\")\n",
        "\n",
        "# Step 8: Normalize all values in the dataset between 0 and 10\n",
        "df_normalized = (df - df.min()) / (df.max() - df.min()) * 10\n",
        "\n",
        "# Step 9: Replace all the values in the dataset with 1 if value <= 0.5 else with 0\n",
        "df_binary = (df_normalized > 0.5).astype(int)\n",
        "\n",
        "# Print the first 5 rows of the final binary dataset\n",
        "print(\"First 5 rows of the final binary dataset:\")\n",
        "print(df_binary.head())\n"
      ],
      "metadata": {
        "id": "-sZSQfm-0LXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q13. Create a random dataset of 500 rows and 10 columns.\n",
        "Columns 1 to 4 are defined between [-10, 10];\n",
        "Columns 5 to 8 are defined between [10, 20];\n",
        "Columns 9 to 10 are defined between [-100, 100].\n",
        "Apply following clustering algorithms; determine the optimal number of clusters and plot distance metric\n",
        "graph using each algorithm.\n",
        "\n",
        "(i) K-Mean clustering\n",
        "\n",
        "(ii) Hierarchical clustering"
      ],
      "metadata": {
        "id": "S7DlQZm0mGOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Generate random dataset\n",
        "np.random.seed(42)\n",
        "data = np.hstack((\n",
        "    np.random.uniform(-10, 10, size=(500, 4)),\n",
        "    np.random.uniform(10, 20, size=(500, 4)),\n",
        "    np.random.uniform(-100, 100, size=(500, 2))\n",
        "))\n",
        "\n",
        "# Create a DataFrame for better handling\n",
        "df = pd.DataFrame(data, columns=[f'col{i}' for i in range(1, 11)])\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "U3kse-ScmUAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means Clustering\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(df)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow Method graph\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8jFOs7ehmogI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Hierarchical Clustering\n",
        "linked = linkage(df, method='ward')\n",
        "\n",
        "# Plot the Dendrogram\n",
        "plt.figure(figsize=(15, 7))\n",
        "dendrogram(linked,\n",
        "           orientation='top',\n",
        "           distance_sort='descending',\n",
        "           show_leaf_counts=True)\n",
        "plt.title('Dendrogram for Hierarchical Clustering')\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JLThzt7mmq-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q14. Create a random dataset of 600 rows and 15 columns. All the values are defined between [-100,100].\n",
        "Perform the following operations:\n",
        "\n",
        "(i) Plot scatter graph between Column 5 and Column 6.\n",
        "\n",
        "(ii) Plot histogram of each column in single graph.\n",
        "\n",
        "(iii) Plot the Box plot of each column in single graph."
      ],
      "metadata": {
        "id": "oYojk2setNOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate random dataset\n",
        "np.random.seed(42)\n",
        "data = np.random.uniform(-100, 100, size=(600, 15))\n",
        "\n",
        "# Create a DataFrame for better handling\n",
        "df = pd.DataFrame(data, columns=[f'col{i}' for i in range(1, 16)])\n",
        "\n",
        "# Plot scatter graph between Column 5 and Column 6\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['col5'], df['col6'], alpha=0.6)\n",
        "plt.title('Scatter Plot between Column 5 and Column 6')\n",
        "plt.xlabel('Column 5')\n",
        "plt.ylabel('Column 6')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot histogram of each column in a single graph\n",
        "df.hist(bins=20, figsize=(15, 15), grid=False, edgecolor='black')\n",
        "plt.suptitle('Histograms of All Columns')\n",
        "plt.show()\n",
        "\n",
        "# Plot the Box plot of each column in a single graph\n",
        "plt.figure(figsize=(15, 8))\n",
        "df.boxplot()\n",
        "plt.title('Box Plots of All Columns')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Values')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kcFAbKEvtH5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q15. Create a random dataset of 500 rows and 5 columns:\n",
        "All the values are defined between [5,10].\n",
        "Perform the following operations:\n",
        "\n",
        "(i) Perform t-Test on each column.\n",
        "\n",
        "(ii) Perform Wilcoxon Signed Rank Test on each column.\n",
        "\n",
        "(iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4"
      ],
      "metadata": {
        "id": "l4xvAwAYtc41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_1samp, wilcoxon, ttest_ind, mannwhitneyu\n",
        "\n",
        "# Generate random dataset\n",
        "np.random.seed(42)\n",
        "data = np.random.uniform(5, 10, size=(500, 5))\n",
        "\n",
        "# Create a DataFrame for better handling\n",
        "df = pd.DataFrame(data, columns=[f'col{i}' for i in range(1, 6)])\n",
        "\n",
        "# Perform t-Test on each column\n",
        "ttest_results = {}\n",
        "for col in df.columns:\n",
        "    t_stat, p_val = ttest_1samp(df[col], popmean=7.5)  # Hypothetical mean is 7.5\n",
        "    ttest_results[col] = (t_stat, p_val)\n",
        "\n",
        "print(\"t-Test Results:\")\n",
        "for col, (t_stat, p_val) in ttest_results.items():\n",
        "    print(f\"{col}: t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\")\n",
        "\n",
        "# Perform Wilcoxon Signed Rank Test on each column\n",
        "wilcoxon_results = {}\n",
        "for col in df.columns:\n",
        "    w_stat, p_val = wilcoxon(df[col] - 7.5)  # Hypothetical median is 7.5\n",
        "    wilcoxon_results[col] = (w_stat, p_val)\n",
        "\n",
        "print(\"\\nWilcoxon Signed Rank Test Results:\")\n",
        "for col, (w_stat, p_val) in wilcoxon_results.items():\n",
        "    print(f\"{col}: W-statistic = {w_stat:.4f}, p-value = {p_val:.4f}\")\n",
        "\n",
        "# Perform Two Sample t-Test on Column 3 and Column 4\n",
        "t_stat, p_val = ttest_ind(df['col3'], df['col4'])\n",
        "print(f\"\\nTwo Sample t-Test between Column 3 and Column 4: t-statistic = {t_stat:.4f}, p-value = {p_val:.4f}\")\n",
        "\n",
        "# Perform Wilcoxon Rank Sum Test on Column 3 and Column 4\n",
        "w_stat, p_val = mannwhitneyu(df['col3'], df['col4'])\n",
        "print(f\"Wilcoxon Rank Sum Test between Column 3 and Column 4: W-statistic = {w_stat:.4f}, p-value = {p_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "cKia1DHetiHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}